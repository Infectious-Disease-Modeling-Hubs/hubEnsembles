---
title: "`hubEnsembles`: Ensembling Methods in R"
output: 
  bookdown::html_document2
bibliography: ../manuscript/references.bib
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.width = 12,
  fig.height = 9
)
```

# Introduction

Predictions of future outcomes are essential to planning and decision making, yet generating reliable predictions of the future is challenging. One method for overcoming this challenge is combining predictions across multiple, independent models. These combination methods (also called aggregation or ensembling) have been repeatedly shown to produce predictions that are more accurate [@clemen1989; @timmermann2006b] and more consistent [@hibon2005] than individual models. Because of the clear performance benefits, multi-model ensembles are commonplace across fields, including weather [@alley2019], climate [@tebaldi2007], and economics [@aastveit2018]. More recently, multi-model ensembles have been used to improve predictions of infectious disease outbreaks [@viboud2018; @johansson2019; @mcgowan2019; @cramer2022]. 

Across this vast literature, there are many proposed methods for generating ensembles. Generally, these methods differ in at least one of two ways: (1) the function used to combine or "average" predictions, and (2) how predictions are weighted when performing the combination. No one method is universally "the best"; a simple average of predictions works surprisingly well across a range of settings [@winkler2015], but more complex approaches have also been shown to have benefits [@mcandrew_aggregating_2021]. Here, we present the `hubEnsembles` package, which provides a flexible framework for generating ensemble predictions from multiple models. Complementing other software for combining predictions from multiple models [@pedregosa_scikit-learn_2011; @weiss2019; @bosse_stackr_2023; @couch_stacks_2023], `hubEnsembles` supports multiple types of predictions from point estimates to probabilistic predictions. Throughout, we will use the term "prediction" to refer to any kind of model output that may be combined including a forecast, a scenario projection, or a parameter estimate.

The `hubEnsembles` package is part of a larger collection of open-source software and data tools that enables collaborative modelling exercises called the "hubverse" (https://hubdocs.readthedocs.io/en/latest/index.html). The broader "hubverse" initiative is motivated by the demonstrated benefits of collaborative hubs [@reich2022], including performance improvements of multi-model ensembles and the desire for standardization across such hubs. In this paper, we focus specifically on the functionality encompassed in `hubEnsembles`. We provide an overview of the methods implemented, including mathematical definitions and properties (Section 2) as well as implementation details (Section 3), we give simple examples to demonstrate the functionality (Section 4) and a more complex case study (Section 5) that motivates a discussion and comparison of the various methods (Section 6).

<!--#EH GENERAL COMMENT: IF YOU HAVE BETTER REFERENCES ANYWHERE, LET ME KNOW.-->

# Mathematical definitions and properties of ensemble methods

The `hubEnsembles` package supports both point predictions and probabilistic predictions of different formats. A point prediction is a single estimate of a future outcome, and a probabilistic prediction gives an estimated probability distribution over future outcomes. We use $N$ to denote the total number of individual predictions that the ensemble will combine. For example, these predictions will often be produced by different statistical or mathematical models, and $N$ is the total number of models. Individual predictions will be indexed by the subscript $i$. Optionally, the package allows for calculating ensembles that use a weight $w_i$ for each prediction. Informally, predictions with a larger weight have a greater influence on the value of the ensemble prediction, though the details of this depend on the ensemble method (described more below). 

For a set of point predictions, $p_i$ each from a distinct model $i$, the `hubEnsembles` package can compute an ensemble of these predictions

$$
p_E = C(p_i, w_i) 
$$

using any function $C$, and model-specific weights $w_i$. For example, an arithmetic average of predictions yields $x_E = \sum_{i=1}^Nx_iw_i$, where the weights are non-negative and sum to 1. If $w_i = 1/N$ for all $i$, all predictions will be equally weighted. This framework can also support more complex functions for aggregation, such as a (weighted) median or geometric mean.

For probabilistic predictions, there are two commonly used classes of methods to average or ensemble multiple predictions: quantile averaging [@lichtendahl2013] (also called a Vincent average [@vincent1912]) and probability averaging [@lichtendahl2013] (also called a distributional mixture or linear opinion pool [@stone1961]).  Let $F(x)$ be a cumulative density function (CDF) defined over values $x$ of the target variable for the prediction, and $F^{-1}(\theta)$ be the corresponding quantile function defined over quantile levels $\theta \in [0, 1]$. Additionally, we will use $f(x)$ to denote a probability mass function (PMF) for a prediction of a discrete variable or a discretization (such as binned values) of a continuous variable.

The quantile average is calculated as
$$
F^{-1}_Q(\theta) = \sum_{i = 1}^Nw_iF^{-1}_i(\theta).
$$
This computes the average value of predictions across different models for each fixed quantile level $\theta$. It is also possible to use other combination functions, such as a weighted median, to combine quantile predictions.

The probability average or linear pool is calculated by averaging probabilities across predictions for a fixed value of the target variable, $x$. This can be expressed in terms of either predictive CDFs or PMFs as follows:
\begin{align*}
F_{LOP}(x) &= \sum_{i = 1}^Nw_iF_i(x), \\
f_{LOP}(x) &= \sum_{i = 1}^Nw_if_i(x).
\end{align*}


The different averaging methods for probabilistic predictions yield different properties of the resulting ensemble distribution. For example, the variance of the linear pool is $\sigma^2_{LOP} = \sum_{i=1}^Nw_i\sigma_i^2 + \sum_{i=1}^Nw_i(\mu_i-\mu_{LOP})^2$, where $\mu_i$ is the mean and $\sigma^2_i$ is the variance of individual prediction $i$, and although there is no closed-form variance for the quantile average, the variance of the quantile average will always be less than or equal to that of the linear pool [@lichtendahl2013]. The linear pool method preserves variation between individual models, whereas the quantile average cancels away this variation [@howerton2023]. Both methods generate distributions with the same mean, $\mu_Q = \mu_{LOP} = \sum_{i=1}^Nw_i\mu_i$, which is the mean of individual model means. <!--#ELR: I didn't know this result for \mu_Q! Can we add a citation or put a derivation in a supplement? -->

# Model implementation details

To understand how these methods are implemented in `hubEnsembles`, we first must define the conventions employed by the hubverse and its packages for representing and working with model predictions. We begin with a short overview of concepts and conventions needed to utilize the `hubEnsembles` package, then explain the implementation of the two ensembling functions provided by the package, `simple_ensemble` and `linear_pool`.

## Hubverse terminology and conventions

Model output is a central concept in the `hubEnsembles` package. Model output generally refers to a specially formatted tabular representation of predictions produced by a modeling team. Each row represents a single, unique prediction with each column providing information about what is being predicted, its scope, and its value. Per hubverse convention, the columns may be broken down into task IDs, specification of the model output representation, and the model ID [@hubverse_docs]. <!--#ELR: or `model_id`, if we want to name the specific column -->

At minimum the task IDs (also called task ID variables) together specify the desired outcome being predicted, but they may also include additional information, such as any conditions or assumptions that were used to generate the predictions [@hubverse_docs]. For example, short-term forecasts of incident influenza hospitalizations in the US at different locations and amounts of time in the future might represent this information using a `target` column with the value "incident flu hospitalizations", a `location` column identifying the location being predicted, a `reference_date` column with the "starting point" of the forecasts, and a `horizon` column with the number of steps ahead that the forecast is predicting relative to the `reference_date`. All of these are task ID columns [@hubverse_docs]. 

Alternatively, longer-term scenario projections for incident COVID-19 hospitalizations in the US at different locations, amounts of time in the future, and under different assumed conditions may use the following task ID columns: a `target` of "incident COVID-19 hospitalizations", a `location` column specifying the location being predicted, an `origin_date` on which the projections were made, a `horizon` describing the number of steps ahead that the forecast is predicting relative to the `origin_date`, and a `scenario_id` denoting under what circumstances would likely result in the protected number of incident hospitalizations. Different modeling efforts may use different sets of task ID columns and values to specify their prediction goals. Additional examples of task ID variables are available on the [hubverse documentation website](https://hubdocs.readthedocs.io/en/latest/user-guide/tasks.html).

The model output representation includes the predicted values along with metadata that specifies how the predictions are conveyed, and consists of three columns: (1) `output_type`, (2) `output_type_id`, and (3) `value`. Unlike for the task IDs, these three columns are required and their names are fixed [@hubverse_docs]. The `output_type` defines how the prediction is represented and may be one of `"mean"` or `"median"` (point forecasts), `"quantile"`, `"cdf"`, `"pmf"` (distributional forecasts), or `"sample"` (although this output type is not yet supported by the `hubEnsembles` package). The `output_type_id` provides more identifying information for a prediction and is specific to the particular `output_type`. For quantile predictions, the `output_type_id` is a numeric value between 0 and 1 specifying the probability level for the quantile. In the notation we defined above, the `output_type_id` corresponds to $\theta$ and the `value` of the prediction is the quantile estimate $F^{-1}(\theta)$. For CDF or PMF predictions, the `output_type_id` is the value $x$ at which the cumulative distribution function or probability mass function for the predictive distribution should be evaluated, and the `value` column contains the estimate $F(x)$ or $f(x)$, respectively. Requirements for the values of the `output_type_id` and `value` columns associated with each valid output type are summarized on the [hubverse documentation website](https://hubdocs.readthedocs.io/en/latest/user-guide/model-output.html#formats-of-model-output) and in the table below.
<!--#ELR: This kind of citation seems fine for an ordinary package vignette, but if we're aiming to directly turn this into a journal submission, we'll need to tidy this citation up a bit. -->
<!--#ELR: I don't love the use of the word "estimate" that I introduced here -- noting that in much of statistics, a distinction is made between an estimate and a prediction, but I've used the two terms semi-interchangeably here. --> 
Finally, the `model_id` column gives a unique identifier of the model that created the predictions. <!--#ELR: Note that I deleted some text here that implied that model_id was not a required column -- but currently, it looks like hubUtils::validate_model_out_tbl will throw an error if model_id is not in the tbl columns. -->

| `output_type` | `output_type_id` | `value` |
|:---|:------------|:------------------------|
| `mean` | NA (not used for mean predictions) | Numeric: The mean of the predictive distribution |
| `median` | NA (not used for median predictions) | Numeric: The median of the predictive distribution |
| `quantile` | Numeric between 0.0 and 1.0: A probability level | Numeric: The quantile of the predictive distribution at the probability level specified by the output_type_id |
| `cdf` | Numeric within the support of the outcome variable: a possible value of the target variable | Numeric between 0.0 and 1.0: The value of the cumulative distribution function of the predictive distribution at the value of the outcome variable specified by the output_type_id |
| `pmf` | String naming a possible category of a discrete outcome variable | Numeric between 0.0 and 1.0: The value of the probability mass function of the predictive distribution when evaluated at a specified level of a categorical outcome variable |
| `sample` | Positive integer sample index | Numeric: A sample from the predictive distribution |

<!-- # LS: Should we include the summary table from the hubverse docs in the paper itself or simply link it? -->
<!--#ELR: I think it could be helpful to include some kind of table here too.  It's a bit much to try to understand how the pieces fit together just from the paragraph in the text...? -->

## Simple ensemble

The `simple_ensemble` function directly computes an ensemble from component model outputs by combining them via some function ($C$) within each unique combination of task ID variables, output types, and output type IDs. This function can be used to summarize predictions of output types mean, median, quantile, CDF, and PMF. The mechanics of the ensemble calculations are the same for each of the output types, though the resulting statistical ensembling method differs for different output types as described below. An aggregation function $C$ of choice may be specified by the user.

By default, `simple_ensemble` uses the mean for the aggregation function $C$ and equal weights for all models. For point predictions with a mean or median output type, the resulting ensemble prediction is an equally weighted average of the individual models' predictions. For probabilistic forecasts in a quantile format, by default `simple_ensemble` produces a quantile average, and for model outputs in a CDF or PMF format, by default `simple_ensemble` computes an equally weighted linear opinion pool.

A median ensemble may also be created by specifying "median" as the aggregation function, or a custom function may be passed to the `agg_fun` argument to create other ensemble types. Similarly, model weights can be specified to create a weighted ensemble. These weights are allowed to vary for different task ID values, and for quantile forecasts the weights may also be different for different quantile probability levels (corresponding to values of the `output_type_id` column).

## Linear pool

The `linear_pool` function implements the linear opinion pool method for ensembling projections. This function can be used to combine predictions with output types mean, quantile, CDF, and PMF. Unlike `simple_ensemble`, this function handles its computation differently based on the output type.

For the CDF, PMF, and mean output types, the linear pool method is equivalent to calling `simple_ensemble` with a mean aggregation function, since `simple_ensemble` produces a linear pool prediction for those output types. 

However, implementation of LOP is less straightforward for the quantile output type. The reason for this is that LOP averages CDF values (probability) at each value of the target variable, but the predictions are quantiles (on the scale of the target variable) at fixed probability levels. The value for these quantile predictions will generally differ between models, and as a result we are typically not provided with CDF values at the same values of $x$ from all component predictions. The lack of alignment between CDF values for the same probability levels is illustrated in the figure below. 

```{r linear-pool-from-quantiles, echo=FALSE, fig.cap="The probabilities and associated values of two normal CDFs (N(100, 10) and N(110, 5)) for nine fixed quantiles are plotted as points. Vertical dashed lines show that the values of these distributions at the fixed probability levels do not align and thus can not be used to calculate LOP. However, the quantiles can be used to obtain the full CDFs, shown by the faint outline behind the points, which then can be used to calculate LOP."}
library(distfromq)
library(ggplot2)
quantile_probs <- seq(from = 0.1, to = 0.9, by = 0.1)
x <- seq(from = 0.0, to = 400.0, length = 1001)

mean1 <- 100
sd1 <- 10
q_normal1 <- qnorm(quantile_probs, mean = mean1, sd = sd1)
cdf_normal1 <- pnorm(x, mean = mean1, sd = sd1)

mean2 <- 120
sd2 <- 5
q_normal2 <- qnorm(quantile_probs, mean = mean2, sd = sd2)
cdf_normal2 <- pnorm(x, mean = mean2, sd = sd2)

dplyr::bind_rows(
  data.frame(x = x, y = cdf_normal1, dist = "N(100, 10)"),
  data.frame(x = x, y = cdf_normal2,dist = "N(120, 5)"),
) |>
    ggplot() +
      geom_line(
        mapping = aes(x = x, y = y, color = dist),
        linewidth = 0.8, alpha = 0.15) +
      geom_point(
        data = dplyr::bind_rows(
          data.frame(q = q_normal1, p = quantile_probs, dist="N(100, 10)"),
          data.frame(q = q_normal2, p = quantile_probs, dist="N(120, 5)")
        ),
        mapping = aes(x = q, y = p, color = dist),
        size = 1.2
      ) +
      geom_vline(
        data = dplyr::bind_rows(
          data.frame(q = q_normal1, dist="N(100, 10)"),
          data.frame(q = q_normal2, dist="N(120, 5)")
        ),
        mapping = aes(xintercept = q, color = dist),
        linewidth = 0.5, alpha = 0.9, linetype = 2
      ) +
      scale_color_viridis_d(
        "Distribution",
        end = 0.9
      ) +
      coord_cartesian(xlim=c(75, 140)) +
      xlab("Value") + ylab("Probability") +
      theme_bw()
```

Given that LOP cannot be directly calculated from quantile predictions, we must first obtain an estimate of the CDF for each component distribution using the provided quantiles, combine the CDFs, then calculate the quantiles from the ensemble's CDF. We perform this calculation in three main steps, assisted by the `distfromq` package [REF] for the first two:

1.  Interpolate and extrapolate from the provided quantiles for each component model to obtain an estimate of the CDF of that particular distribution.
2.  Draw samples from each component model distribution. To reduce Monte Carlo variability, we use quasi-random samples corresponding to quantiles of the estimated distribution [@niederreiter1992quasirandom].
3.  Pool the samples from all component models and extract the desired quantiles.

For step 1, functionality in the `distfromq` package uses a monotonic cubic spline for interpolation on the interior of the provided quantiles. The user may choose one of several distributions to perform extrapolation of the CDF tails. These include normal, lognormal, and cauchy distributions, with "normal" set as the default. A location-scale parameterization is used, with separate location and scale parameters chosen in the lower and upper tails so as to match the two most extreme quantiles.

# Demonstration of functionality

In this section, we illustrate the two main functions in `hubEnsembles`, `simple_ensemble` and `linear_pool`. This vignette uses the following R packages:

```{r setup}
library(dplyr)
library(tidyr)
library(ggplot2)
library(scales)
library(cowplot)
library(hubUtils)
library(hubEnsembles)
```


## Example data: a simple forecast hub

The `example-simple-forecast-hub` has been created by the Consortium of Infectious Disease Modeling Hubs as a simple example hub to demonstrate the set up and functionality for the hubverse. The hub includes both example model output data and target data. The model output data includes `quantile`, `mean` and `median` forecasts of future incident hospitalizations, as well as `pmf` forecasts of the probability that the change in hospitalizations will be a "large decrease", "decrease", "stable", "increase", "large increase". 

<!-- # EH: A few notes on this: 
(1) I modified the example hub data to include mean and pmf output types. The mean output type was estimated from 1E4 samples (using distfromq package). The pmf output type was modeled after FluSight (probability of "large_decrease", "decrease", "stable", etc. but the values were randomly generated from a uniform distribution. Does this sound okay? Is it a problem that this won't match the example-hub directly?

(2) As of now, I have not included the cdf output type for inc hosp forecasts because it is difficult to define standard bins. I do, however, see value in including an example with cdf output type. So two potential options for that: 
    (a) include another target (e.g., peak timing) of cdf output type in this example hub 
    (b) create a very simple example, say with normal distributions, that will illustrate the fundamental differences between quantile average and linear pool (similar to Fig 2 in my paper, https://royalsocietypublishing.org/doi/10.1098/rsif.2022.0659) I have always found this figure useful, but maybe it is just me! I also think this option could help reinforce the connection between the methods described in the Mathematical Definitions section, and the implementation of each function (i.e., simple_ensemble() applied to quantile is a different method than simple_ensemble() applied to cdf/pmf).
) -->

<!-- # ELR:
(1) I think this is OK. We could grab updates forecasts (including categorical targets) from the new flu forecast hub if we wanted to, though. This would not be hard, but would require re-working some aspects of this set up?

(2) Your suggestion makes sense to me. We actually already have an example like this as part of the unit tests for the linear pool, here: https://github.com/Infectious-Disease-Modeling-Hubs/hubEnsembles/blob/80517854361312b81bce43dddf4d767d3a793b45/tests/testthat/test-linear_pool.R#L167.  I also find that plot helpful, and we might also be able to use it to help illustrate the issue with creating a linear pool based on quantile forecasts that I noted above.
-->

First we load the example model output data and the target data using the `connect_hub()` function from `hubUtils`, another package developed by the Consortium of Infectious Disease Modeling Hubs. 

```{r}
hub_path <- system.file("example-data/example-simple-forecast-hub",
                        package = "hubEnsembles")

model_outputs <- hubUtils::connect_hub(hub_path) %>%
  dplyr::collect()
head(model_outputs)

target_data_path <- file.path(hub_path, "target-data",
                              "covid-hospitalizations.csv")
target_data <- read.csv(target_data_path) %>%
    mutate(time_idx = as.Date(time_idx))
head(target_data)
```

## Creating ensembles with `simple_ensemble`

We can generate an equally weighted mean ensemble for each unique combination of values for the task ID variables (here, `origin_date`, `horizon`, `location`, and `target`), the `output_type` and the `output_type_id`. Note that this means that different ensemble methods are used for different output types: for the `quantile` output type in our example data, the resulting ensemble is a quantile average, while for the `pmf` output type, the ensemble is a linear pool.

```{r}
mean_ens <- hubEnsembles::simple_ensemble(model_outputs)
head(mean_ens)
```

### Changing the aggregation function

We can change the function that is used to aggregate model outputs. For example, we may want to calculate a median of component model submitted values for each quantile. We do so by specifying `agg_fun = median`. We will also use `model_id = "hub-ensemble-median` to change the name of this ensemble in the resulting data frame.

```{r}
median_ens <- hubEnsembles::simple_ensemble(model_outputs, 
                                            agg_fun = median, 
                                            model_id = "hub-ensemble-median")
head(median_ens)
```
Custom functions can also be passed into the `agg_fun` argument. For example, in some circumstances a geometric mean may be a more appropriate way to combine component model outputs; here we define a custom function `geometric_mean()` to do so. Any custom function to be used requires an argument `x` for the vector of numeric values to summarize, and if relevant, an argument `w` of numeric weights. Then, we can use this custom function to ensemble the component model outputs, again using `agg_fun = geometric_mean`.

```{r}
geometric_mean <- function(x){
    n <- length(x)
    return(prod(x)^(1/n))
}

geometric_mean_ens <-  hubEnsembles::simple_ensemble(model_outputs, 
                                                     agg_fun = geometric_mean, 
                                                     model_id = "hub-ensemble-geometric")
head(geometric_mean_ens)
```

As expected, the mean, median, and geometric mean each give us slightly different resulting ensembles. The median point estimates in Figure XX demonstrate this. 

```{r}
plt_all_point_ens <- bind_rows(mean_ens, 
                               median_ens, 
                               geometric_mean_ens) %>% 
    filter(location == "US", 
           origin_date == "2022-12-12", 
           output_type == "median") %>%
    mutate(target_date =  origin_date + horizon, 
           ens_flag = model_id)

plt_all_point_mod <- model_outputs %>% 
    filter(location == "US", 
           origin_date == "2022-12-12", 
           output_type == "median") %>%
    mutate(target_date =  origin_date + horizon)

ggplot(data = plt_all_point_ens) + 
    geom_point(data = target_data %>%
                   filter(location == "US", 
                          time_idx >= "2022-10-01",
                          time_idx <= "2023-03-01"),
               aes(x = time_idx, y = value)) +
    geom_line(data = target_data %>%
                   filter(location == "US", 
                          time_idx >= "2022-10-01",
                          time_idx <= "2023-03-01"),
               aes(x = time_idx, y = value)) +
    geom_vline(aes(xintercept = as.Date("2022-12-06"))) +
    geom_line(data = plt_all_point_mod, 
              aes(x = target_date, y = value, color = model_id), linewidth = 0.9) + 
    geom_line(aes(x = target_date, y = value, color = model_id), linewidth = 0.9) + 
    facet_grid(rows = vars(ens_flag)) +
    scale_color_manual(values = c(rainbow(3), gray(seq(0.2, 0.8,length.out = 3)))) +
    scale_y_continuous(labels = comma, 
                       name = "US incident hospitalizations") +
    theme_bw() + 
    theme(axis.title.x = element_blank(), 
          legend.position = "bottom", 
          legend.title = element_blank())
```

<!-- # EH: I don't love the color scheme here... it's especially hard to see the baseline model. -->

### Weighting model contributions

In addition, we can weight the contributions of each model by providing a `data.frame` that specifies these weights. For example, if we wanted to include the baseline model in the ensemble, but give it less weight than the other forecasts, we would use the `weights = model_weights` argument, where `model_weights` is a `data.frame` with a `model_id` column containing each unique model_id and a `weight` column.

```{r}
model_weights <- data.frame(model_id = c("UMass-ar", "UMass-gbq", "simple_hub-baseline"), 
                            weight = c(0.4, 0.4, 0.2))

weighted_mean_ens <- hubEnsembles::simple_ensemble(model_outputs, 
                                                   weights = model_weights, 
                                                   model_id = "hub-ensemble-weighted-mean")
head(weighted_mean_ens)
```
If the `data.frame` defines model weights in a column named something other than `weight`, we can specify this using the `weights_col_name` argument. <!-- ELR: describing this in the vignette feels low priority to me -- candidate for removal? -->

```{r}
model_weights <- data.frame(model_id = c("UMass-ar", "UMass-gbq", "simple_hub-baseline"), 
                            w = c(0.4, 0.4, 0.2))

weighted_mean_ens <- hubEnsembles::simple_ensemble(model_outputs, 
                                                   weights = model_weights, 
                                                   weights_col_name = "w",
                                                   model_id = "hub-ensemble-weighted-mean")
head(weighted_mean_ens)
```

## Creating ensembles with `linear_pool`

We can also generate a linear pool ensemble, or distributional mixture, using the `linear_pool()` function; this function can only be applied to predictions with an `output_type` of `mean`, `quantile`, `cdf`, or `pmf`. Our example hub includes `median` output type, so we exclude it from the calculation. 

```{r}
linear_pool_ens <- hubEnsembles::linear_pool(model_outputs %>%
                                                 filter(output_type != "median"), 
                                             model_id = "hub-ensemble-linear-pool")
head(linear_pool_ens)
```

As described above, for `quantile` model outputs, the `linear_pool` function approximates the full probability distribution for each component prediction using the value-quantile pairs provided by that model model, and then obtains quasi-random samples from that distributional estimate. The number of samples drawn from the distribution of each component model defaults to `1e4`, but this can be changed using the `n_samples` argument. 

For the `mean`, `cdf` and `pmf` output types, the linear pool is equivalent to using a mean `simple_ensemble` (see [Mathematical definitions and properties of ensemble methods] for explanation). For `quantile` output types, the `linear_pool` function yields an ensemble with wider prediction intervals than `simple_ensemble`. <!-- ELR: consider dropping this paragraph since it duplicates stuff that's been said before? -->

```{r}
plt_model_outputs = model_outputs %>%
    filter(location == "US", 
           origin_date == "2022-12-12", 
           output_type == "quantile", 
           output_type_id %in% c(0.05, 0.25, 0.5, 0.75, 0.95)) %>%
    mutate(target_date =  origin_date + horizon, 
           output_type_id = paste0("Q", as.numeric(output_type_id)*100)) %>%
    spread(output_type_id, value)
    
plt_ens_outputs <- bind_rows(
    mean_ens %>% 
        filter(location == "US", 
               origin_date == "2022-12-12", 
               output_type == "quantile", 
               output_type_id %in% c(0.05, 0.25, 0.5, 0.75, 0.95)) %>%
        mutate(target_date =  origin_date + horizon, 
               output_type_id = paste0("Q", as.numeric(output_type_id)*100), 
               ens_fn = "simple_ensemble"), 
    linear_pool_ens %>% 
        filter(location == "US", 
               origin_date == "2022-12-12", 
               output_type == "quantile", 
               output_type_id %in% c(0.05, 0.25, 0.5, 0.75, 0.95)) %>%
        mutate(target_date =  origin_date + horizon, 
               output_type_id = paste0("Q", as.numeric(output_type_id)*100), 
               ens_fn = "linear_pool")
) %>%
    spread(output_type_id, value)

lims = c(min(c(plt_model_outputs$Q5, plt_ens_outputs$Q5)), 
         max(c(plt_model_outputs$Q95, plt_ens_outputs$Q95)))

p1 = ggplot(data = plt_model_outputs, 
            aes(x = target_date)) + 
    geom_point(data = target_data %>%
                   filter(location == "US", 
                          time_idx >= "2022-10-01",
                          time_idx <= "2023-03-01"),
               aes(x = time_idx, y = value)) +
    geom_line(data = target_data %>%
                  filter(location == "US", 
                         time_idx >= "2022-10-01",
                         time_idx <= "2023-03-01"),
              aes(x = time_idx, y = value)) +
    geom_vline(aes(xintercept = as.Date("2022-12-06"))) +
    geom_ribbon(aes(ymin = Q5, ymax = Q95), alpha = 0.2) + 
    geom_ribbon(aes(ymin = Q25, ymax = Q75), alpha = 0.2) + 
    geom_line(aes(y = Q50), size = 0.9) +
    facet_wrap(vars(model_id), ncol = 1)+
    scale_y_continuous(labels = comma, 
                       limits = lims,
                       name = "US incident hospitalizations") +
    theme_bw() +
    theme(axis.title.x = element_blank(), 
          legend.position = "none", 
          legend.title = element_blank())
    
p2 = ggplot(data = plt_ens_outputs, 
            aes(x = target_date)) + 
    geom_point(data = target_data %>%
                   filter(location == "US", 
                          time_idx >= "2022-10-01",
                          time_idx <= "2023-03-01"),
               aes(x = time_idx, y = value)) +
    geom_line(data = target_data %>%
                   filter(location == "US", 
                          time_idx >= "2022-10-01",
                          time_idx <= "2023-03-01"),
               aes(x = time_idx, y = value)) +
    geom_vline(aes(xintercept = as.Date("2022-12-06"))) +
    geom_ribbon(aes(ymin = Q5, ymax = Q95, fill = model_id), alpha = 0.2) + 
    geom_ribbon(aes(ymin = Q25, ymax = Q75, fill = model_id), alpha = 0.2) + 
    geom_line(aes(y = Q50, color = model_id), size = 0.9) +
    scale_y_continuous(labels = comma, 
                       limits = lims,
                       name = "US incident hospitalizations") +
    theme_bw() +
    theme(axis.title.x = element_blank(), 
          legend.position = "bottom", 
          legend.title = element_blank())

plot_grid(p1, p2, 
          align = "h", axis = "b")
```


# Case study: Weekly incident flu hospitalizations

To demonstrate the utility of the `hubEnsembles` package and the differences between the two ensembling functions, we examine the case of predicting weekly influenza hospitalizations in the US. 

Since 2013 the US Center for Disease Control and Prevention (CDC) has been soliciting forecasts of seasonal influenza from modeling teams through a collaborative challenge called FluSight [@cdc_flusight]. Here we combine these forecasts using various aggregation methods to take advantage of the greater consistency [@hibon2005] and accuracy [@clemen1989; @timmermann2006b] of ensembles over individual models. In particular, we examine four equally-weighted ensembling methods implemented through `simple_ensemble()` and `linear_pool()`: a quantile (arithmetic) mean, a quantile median, a linear pool with normal tails, and a linear pool with lognormal tails.

The component forecasts used to generate the ensembles consist of those submitted during influenza seasons 2021-2022 and 2022-2023, the only two complete seasons for which FluSight collected quantile forecasts for a target of weekly incident flu hospitalizations at the time of this writing. These predictions are not yet formatted according to hubverse standards but are easily transformed using the `as_model_out_tbl()` function from the `hubUtils` package. Below we print the first six rows of the transformed forecasts made on May 15, 2023 for California.

```{r transform data, eval=TRUE}
flu_forecasts_raw <- readr::read_rds("data/flu_forecasts_raw.rds")

forecast_data_hub <- flu_forecasts_raw |>
  tidyr::separate(target, sep=" ", convert=TRUE, into=c("horizon", "target"), extra="merge") |>
  as_model_out_tbl(
    model_id_col = "model",
    output_type_col = "class",
    output_type_id_col = "quantile",
    value_col = "value",
    sep = "-",
    trim_to_task_ids = FALSE,
    hub_con = NULL,
    task_id_cols = c("forecast_date", "location", "horizon", "target"),
    remove_empty = TRUE
  )
  
forecast_data_hub |>
  dplyr::filter(unit == "06") |>
  head() |>
  knitr::kable()
```

Within the now-properly formatted model outputs, the task ID variables are horizon, location, target, and forecast date (the date on which the forecast was made). All of the forecasts have a quantile output type with 23 total unique output type IDs $$Q = \{.010, 0.025, .050, .100, \cdots, .900, .950, .990\}.$$ The values of these quantile forecasts are non-negative. The resulting ensemble forecasts will have the same task ID variables and model output specifications.

Next the component model outputs are combined using the following code to generate each model, then repeated for every forecast date.

```{r construct ensembles, eval=FALSE}
mean_ensemble <- forecast_data_hub |>
  filter(model_id != "Flusight-baseline") |>
  hubEnsembles::simple_ensemble(weights=NULL, agg_fun = "mean", model_id="mean-ensemble") 

median_ensemble <- forecast_data_hub |>
  filter(model_id != "Flusight-baseline") |>
  hubEnsembles::simple_ensemble(weights=NULL, agg_fun = "median", model_id="median-ensemble")
  
lp_normal <- forecast_data_hub |>
  filter(model_id != "Flusight-baseline") |>
  hubEnsembles::linear_pool(weights=NULL, n_samples = 1e5, model_id="lp-normal", tail_dist="norm") 

lp_lognormal <- forecast_data_hub |>
  filter(model_id != "Flusight-baseline") |>
  hubEnsembles::linear_pool(weights=NULL, n_samples = 1e5, model_id="lp-lognormal", tail_dist="lnorm") 
```

```{r read in forecasts and scores, echo=FALSE}
library(lubridate)
library(patchwork)
library(RColorBrewer)
source("evaluation_functions.R")
source("as_covid_hub_forecasts.R")

flu_truth_all <- readr::read_rds("data/flu_truth_all.rds")
flu_files <- list.files(path="data", pattern="hub", full.names=TRUE)
flu_forecasts_og <- purrr::map_dfr(flu_files, .f=readr::read_rds)
flu_forecasts_all <- purrr::map_dfr(flu_files, .f=readr::read_rds) |>
  as_covid_hub_forecasts()

flu_scores_baseline <- readr::read_rds("data/flu_baseline_scores.rds")
flu_scores_ensembles <- readr::read_rds("data/flu_scores_ensembles.rds")
flu_scores_all <- rbind(flu_scores_ensembles, flu_scores_baseline)

flu_dates_21_22 <- as.Date("2022-01-24") + weeks(0:21)
flu_dates_22_23 <- as.Date("2022-10-17") + weeks(0:30)
flu_dates_off_season <- as.Date("2022-06-27") + weeks(0:15)
all_flu_dates <- c(flu_dates_21_22, flu_dates_22_23)
```

We score the ensemble forecasts for every unique combination of task ID variables using several common metrics in forecast evaluation, including weighted interval score (WIS) [@bracher_evaluating_2021], mean absolute error (MAE), 50% prediction interval (PI) coverage, and 95% PI coverage. Of these metrics, WIS and PI coverage evaluate probabilistic forecasts while MAE evaluates point forecasts. In this analysis, we take the 0.5 quantile to be the point forecast. 

WIS measures how consistent a set of prediction intervals is with the true value and is an alternative to common proper scoring rules like the Log Score and Continuous Ranked Probability Score, which can't be evaluated directly for quantile forecasts [@bracher_evaluating_2021]. Since WIS is made up of three component penalties—one for each of spread, over-prediction, and under-prediction—a lower value indicates a more accurate forecast [@bracher_evaluating_2021]. The $(1-\alpha)*100$% PI coverage measures the proportion of the time that PIs at that nominal level included the true value, which provides information about whether a forecast has accurately characterized the uncertainty of future observations. Achieving approximately nominal ($(1-\alpha)*100$%) coverage indicates a well-calibrated forecast. MAE measures the average absolute error of a set of forecasts against the true value; smaller values of MAE indicate better forecast accuracy.

<!-- ELR: commenting out the following text.  Rather than saying "we made tables and figures with results", let's say things like "The results show that XX model was better than YY model (Table 1, Fig 2)".   "The scores are then grouped together and averages are taken of these metrics over different combinations of five variables: none, season, horizon, forecast date, and location. We display a table of the overall results (no grouping) and figures showing average WIS, MAE, and 95% PI coverage plotted broken down by forecast date, horizon, and season." -->

The results show that the quantile median ensemble had the best overall performance in terms of WIS and MAE (and the relative versions of these metrics) with above-nominal coverage rates (Table \@ref(tab:overall-evaluation)). The two linear opinion pools had very similar performance to each other. These methods had the second-best performance as measured by WIS and MAE, but they had the highest 50% and 95% coverage rates, with empirical coverage that was well above the nominal coverage rate. The quantile mean performed the worst of the ensembles with the highest MAE, which was substantially different from that of the other ensembles.

```{r overall-evaluation, message=FALSE, warning=FALSE, echo=FALSE}
source("format_cells.R")
flu_overall_states <- flu_scores_all |>
  evaluate_flu_scores(grouping_variables=NULL, baseline_name="Flusight-baseline", us_only=FALSE)

flu_overall_states |>
  format_cells(rows=1, cols=c(1:3, 5:7), "bold") |>
  format_cells(rows=4, cols=4, "bold") |>
  format_cells(rows=1, cols=4, "italics") |>
  knitr::kable(caption="Summary of overall model performance across both seasons, averaged over all locations except the US national location.")
  
knitr::kable(flu_overall_states, caption="Summary of overall model performance across both seasons, averaged over all locations except the US national location.")
```

Plots of the models' forecasts can aid our understanding about the origin of these accuracy differences. For example, the linear opinion pools, which had the highest coverage rates, consistently had some of the widest prediction intervals. The median ensemble, which had the best WIS, seems to have best-balanced interval width overall, with narrower intervals than the linear pools that achieved near-nominal coverage on average across all time points. The quantile mean's interval widths could vary, though it usually had narrower intervals than the linear pools. However, this model's point forecasts demonstrated a larger error margin compared to the other ensembles, especially at longer horizons. This can be seen in Figure \@ref(fig:plot-forecasts) for the 4-week ahead forecast in California following the 2022-23 season peak on December 5, 2022. Here the quantile mean is predicting a continued increase in hospitalizations while the other models predict a flat or slightly downward trend.

<!-- ELR: comments on the plot below:
 - can we get it so that there is not a line during the off season in this plot?
 - I don't think we need to color by model or have a legend for model color, since the model names are already in the facet labels.
-->

```{r plot-forecasts, echo = FALSE, fig.cap = 'One to four week ahead forecasts for select dates plotted against truth data for Vermont and California.'}
flu_forecasts_wide <- flu_forecasts_all |>
  dplyr::left_join(covidHubUtils::hub_locations_flusight, by = c("location" = "fips")) |>
  dplyr::mutate(horizon=as.character(horizon)) 

select_dates <- all_flu_dates[seq(2, 53, 4)]

forecasts_vt <- flu_forecasts_wide %>% 
  filter(location == "50", forecast_date %in% select_dates)
    
vt_plot <- covidHubUtils::plot_forecasts(forecasts_vt, 
                    hub = "FluSight",
                    intervals = c(.50, .95),
                    truth_data = select(flu_truth_all, model, location, target_end_date, target_variable, value),
                    truth_source = "HealthData",
                    use_median_as_point = TRUE,
                    facet = model ~.,
                    facet_nrow = 5,
                    fill_by_model = TRUE, 
                    fill_transparency = 0.75,
#                    top_layer = c("forecasts", "truth"),
                    title = "none",
                    subtitle = "none",
                    show_caption = FALSE,
                    plot=FALSE)

max_truth_vt <- max(filter(flu_truth_all, location=="50")$value)
max_forecast_vt <- max(filter(forecasts_vt, location=="50")$value)
  
vt_plot <- vt_plot +
  scale_x_date(name=NULL, limits = c(as.Date("2022-01-01"), as.Date("2023-06-08")), date_breaks = "4 months", date_labels = "%b '%y") +
  coord_cartesian(ylim = c(0, max(25, min(max_truth_vt * 3.5, max_forecast_vt)))) +
  ggtitle("Vermont") +
  theme(axis.ticks.length.x = unit(0.5, "cm"),
        axis.text.x = element_text(vjust = 7, hjust = -0.2),
        legend.position = "none")
          

forecasts_ca <- flu_forecasts_wide %>% 
  filter(location == "06", forecast_date %in% select_dates)
    
ca_plot <- covidHubUtils::plot_forecasts(forecasts_ca, 
                    hub = "FluSight",
                    intervals = c(.50, .95),
                    truth_data = select(flu_truth_all, model, location, target_end_date, target_variable, value),
                    truth_source = "HealthData",
                    use_median_as_point = TRUE,
                    facet = model ~.,
                    facet_nrow = 5,
                    fill_by_model = TRUE, 
#                    fill_transparency = 0.75,
#                    top_layer = c("forecasts", "truth"),
                    title = "none",
                    subtitle = "none",
                    show_caption = FALSE,
                    plot=FALSE)

max_truth_ca <- max(filter(flu_truth_all, location=="06")$value)
max_forecast_ca <- max(filter(forecasts_ca, location=="06")$value)
  
ca_plot <- ca_plot +
  scale_x_date(name=NULL, limits = c(as.Date("2022-01-01"), as.Date("2023-06-08")), date_breaks = "4 months", date_labels = "%b '%y") +
  coord_cartesian(ylim = c(0, max(25, min(max_truth_ca * 3.5, max_forecast_ca)))) +
  ggtitle("California") +
  theme(axis.ticks.length.x = unit(0.5, "cm"),
        axis.text.x = element_text(vjust = 7, hjust = -0.2),
        legend.position = "none")
          
vt_plot + ca_plot +
  plot_layout(ncol = 2, guides='collect') &
  plot_annotation(
    title="Weekly Influenza Hospitalizations: observed and forecasted",
    subtitle="Forecasts for Vermont and California, every 4 weeks",
  ) &
  theme(legend.position='none')
```

```{r plot-forecasts-hubVis, echo = FALSE}
library(hubVis)
forecasts_ca <- flu_forecasts_og |> 
  dplyr::filter(location == "06", forecast_date %in% select_dates) 
#  dplyr::mutate(value = ifelse(forecast_date %in% select_dates, value, NA))

truth_ca <- flu_truth_all |> 
  dplyr::filter(location == "06")
  
ca_plot_new <- plot_step_ahead_model_output(
  forecasts_ca,
  truth_ca,
  use_median_as_point=TRUE,
  show_plot=FALSE,
  x_col_name="target_end_date",
  x_truth_col_name = "target_end_date",
  show_legend=FALSE,
  facet="model_id",
  facet_nrow=3,
  interactive=FALSE,
  fill_transparency = 0.45,
  intervals=c(0.5, 0.95),
  title="Weekly Incident Hospitalizations for Influenza in California"
)

max_truth_ca <- max(filter(flu_truth_all, location=="06")$value)
max_forecast_ca <- max(filter(forecasts_ca, location=="06")$value)
  
ca_plot_new <- ca_plot_new +
  scale_x_date(name=NULL, limits = c(as.Date("2022-01-01"), as.Date("2023-06-08")), date_breaks = "4 months", date_labels = "%b '%y") +
  coord_cartesian(ylim = c(0, max(25, min(max_truth_ca * 3.5, max_forecast_ca)))) +
#  ggtitle("California") +
  theme(axis.ticks.length.x = unit(0.5, "cm"),
        axis.text.x = element_text(vjust = 7, hjust = -0.2),
        legend.position = "none")

ca_plot_new
```

When examining the forecasts week-by-week, the ensemble models tend to have similar MAE values during the entire time period. This is especially prevalent at the one-week ahead horizon, with slight divergence in MAE values for certain weeks at the four week ahead horizon Figure \@ref(fig:wis-mae-vs-forecast-date). However, the models show greater differences for the other two metrics, particularly during times of rapid change (Figure \@ref(fig:wis-mae-vs-forecast-date)), with the scores aligning near-perfectly with the true weekly incident hospitalizations. In fact, the linear pools have a lower WIS than the median ensemble at the one week ahead forecast horizon for over a third of forecast dates (11 weeks) during the 2022-2023 season: from October 17, 2022 to December 12, 2022; January 2, 2023; and January 9, 2023. These dates span the rapid rise and fall of incident hospitalizations due to influenza surrounding the season's peak, with the largest differences in WIS occurring on November 28, December 5, December 12, January 2, and January 9. Additionally, the PI coverage rates for the linear pools were at least as large as the coverage rates of the other models throughout the entire period of analysis at both the 1 and 4 week ahead forecast horizons (see Figure \@ref(fig:cov95-vs-forecast-date)).

```{r wis-mae-vs-forecast-date, message=FALSE, warning=FALSE, fig.cap='Average h-week ahead WIS and MAE by forecast date for each model for all locations (except the US national location), split by season for readability. Average truth data across all locations is plotted in black.', echo=FALSE}  
model_names <- c("Flusight-baseline", "lp-lognormal", "lp-normal", "mean-ensemble", "median-ensemble", "average truth")
#model_colors <- c("#6BAED6", "#FD8D3C", "#74C476", "#9E9AC8", "#FB6A4A")
model_colors = c(brewer.pal(5,'Set2'), "black");

flu_date_horizon_season_states <- flu_scores_all |>
  evaluate_flu_scores(grouping_variables=c("horizon", "forecast_date", "season"), baseline_name="Flusight-baseline", us_only=FALSE)

# WIS
wis_date_plot_states1_2122 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2021-2022") |>
  plot_evaluated_scores_forecast_date(model_names, model_colors, h=1, main="WIS 2021-2022, 1 week ahead", truth_data=flu_truth_all, truth_scaling=0.1)
wis_date_plot_states4_2122 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2021-2022") |>
  plot_evaluated_scores_forecast_date(model_names, model_colors, h=4, main="WIS 2021-2022, 4 week ahead", truth_data=flu_truth_all, truth_scaling=0.1)
  
wis_date_plot_states1_2223 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2022-2023") |>
  plot_evaluated_scores_forecast_date(model_names, model_colors, h=1, main="WIS 2022-2023, 1 week ahead", truth_data=flu_truth_all, truth_scaling=0.1)
wis_date_plot_states4_2223 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2022-2023") |>
  plot_evaluated_scores_forecast_date(model_names, model_colors, h=4, main="WIS 2022-2023, 4 week ahead", truth_data=flu_truth_all, truth_scaling=0.1)

# MAE
flu_date_horizon_season_states <- flu_scores_all |>
    evaluate_flu_scores(grouping_variables=c("horizon", "forecast_date", "season"), baseline_name="Flusight-baseline", us_only=FALSE)

mae_date_plot_states1_2122 <- flu_date_horizon_season_states |>
    dplyr::filter(season == "2021-2022") |>
    plot_evaluated_scores_forecast_date(model_names, model_colors, h=1, y_var="mae", main="MAE 2021-2022, 1 week ahead", truth_data=flu_truth_all, truth_scaling=0.15)
mae_date_plot_states4_2122 <- flu_date_horizon_season_states |>
    dplyr::filter(season == "2021-2022") |>
    plot_evaluated_scores_forecast_date(model_names, model_colors, h=4, y_var="mae", main="MAE 2021-2022, 4 week ahead", truth_data=flu_truth_all, truth_scaling=0.15)

mae_date_plot_states1_2223 <- flu_date_horizon_season_states |>
    dplyr::filter(season == "2022-2023") |>
    plot_evaluated_scores_forecast_date(model_names, model_colors, h=1, y_var="mae", main="MAE 2022-2023, 1 week ahead", truth_data=flu_truth_all, truth_scaling=0.15)
mae_date_plot_states4_2223 <- flu_date_horizon_season_states |>
    dplyr::filter(season == "2022-2023") |>
    plot_evaluated_scores_forecast_date(model_names, model_colors, h=4, y_var="mae", main="MAE 2022-2023, 4 week ahead", truth_data=flu_truth_all, truth_scaling=0.15)

# Figure
wis_date_plot_states1_2122 + mae_date_plot_states1_2122 + 
  wis_date_plot_states4_2122 + mae_date_plot_states4_2122 + 
  wis_date_plot_states1_2223 + mae_date_plot_states1_2223 + 
  wis_date_plot_states4_2223 + mae_date_plot_states4_2223 + 
  plot_layout(ncol=2, guides='collect') &
  theme(legend.position='bottom')
```  

```{r cov95-vs-forecast-date, message=FALSE, warning=FALSE, fig.cap='Average h-week ahead 95% PI coverage and truth by forecast date for each model for all locations (except the US national location), split by season for readability.', echo=FALSE}  
# 95% Coverage
cov95_date_plot_states1_2122 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2021-2022") |>
  plot_evaluated_scores_forecast_date(model_names, model_colors, h=1, y_var="cov95", main="95% Coverage 2021-22, 1 week ahead", truth_data=flu_truth_all) +
  coord_cartesian(ylim = c(0, 1.05))
cov95_date_plot_states4_2122 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2021-2022") |>
  plot_evaluated_scores_forecast_date(model_names, model_colors, h=4, y_var="cov95", main="95% Coverage 2021-22, 4 week ahead", truth_data=flu_truth_all) +
  coord_cartesian(ylim = c(0, 1.05))
  
cov95_date_plot_states1_2223 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2022-2023") |>
  plot_evaluated_scores_forecast_date(model_names, model_colors, h=1, y_var="cov95", main="95% Coverage 2022-23, 1 week ahead", truth_data=flu_truth_all) +
  coord_cartesian(ylim = c(0, 1.05))
cov95_date_plot_states4_2223 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2022-2023") |>
  plot_evaluated_scores_forecast_date(model_names, model_colors, h=4, y_var="cov95", main="95% Coverage 2022-23, 4 week ahead", truth_data=flu_truth_all) +
  coord_cartesian(ylim = c(0, 1.05))
    
truth_states_2122 <- plot_flu_truth(flu_truth_all, date_range=flu_dates_21_22[c(1, 22)], main="truth data 2021-22")
truth_states_2223 <- plot_flu_truth(flu_truth_all, date_range=flu_dates_22_23[c(1, 31)], main="truth data 2022-23")

truth_states_2122 + truth_states_2223 + 
  cov95_date_plot_states1_2122 + cov95_date_plot_states1_2223 + 
  cov95_date_plot_states4_2122 + cov95_date_plot_states4_2223 + 
  plot_layout(ncol=2, guides='collect') &
  theme(legend.position='bottom')
```

<!-- # ELR: I'm not getting much out of this paragraph.  Commenting out for now... "Grouping the scores by horizon on its own and horizon in conjunction with season reveal similar patterns in model rankings, with the quantile median achieving the lowest WIS except at the 1 week ahead horizon. When the scores are grouped by only location, the quantile median has the lowest relative WIS for all but three locations, two of which demonstrate inferior performance compared to the baseline and one which has worse performance than the quantile mean. The linear pools outperform the quantile mean ensemble for 75% of locations and perform the same for about 8%, each of these three models outperforming the baseline for approximately 75% of locations." -->

From these results, we can see that different ensembling methods perform best under different circumstances, though in this analysis all of the ensemble variations outperformed the baseline model. While the quantile median had the best overall results for WIS, MAE, 50% PI coverage, and 95% PI coverage, other models may perform better from week-to-week for each metric. For example, the linear pools achieved the lowest WIS during the period of rapid change in incident flu hospitalizations surrounding the season's peak in early December 2022.

Depending on the target being forecast or the goal of forecasting, the user may opt to choose a particular aggregation method that is most aligned with the objectives. One case may call for prioritizing above-nominal coverage rates while another may be more interested in accurate point forecasts. The `simple_ensemble` and `linear_pool` functions and the ability to specify component model weights and an aggregation function for `simple_ensemble` allow users to implement a variety of ensemble methods.


# Conclusion
<!-- # EH: I opted for short and sweet here. We could also expand each of the ideas into a paragraph. -->

Ensembles of independent models are a powerful tool to generate more accurate and more reliable forecasts of future outcomes than a single model alone. Here, we have demonstrated how to utilize `hubEnsembles`, a simple and flexible framework to combine individual model forecasts and create ensemble predictions. When using `hubEnsembles`, it is important to carefully choose an ensemble method that is well suited for the situation. Although there may not be a "best" method, matching the properties of a given ensemble method with the features of the component models will likely yield best results. For example, we showed for forecasts of seasonal influenza in the US, the quantile median ensemble performed best overall, but the linear pool method had advantages during periods of rapid change, when outlying component forecasts were likely more important. Importantly, all ensemble methods outperformed the baseline model. These performance improvements from ensemble models motivate the use of a "hub-based" approach to prediction for infectious diseases and in other fields. Fitting within the larger suite of "hubverse" tools that support such efforts, the `hubEnsembles` package provides important software infrastructure for leveraging the power of multi-model ensembles.

# References
