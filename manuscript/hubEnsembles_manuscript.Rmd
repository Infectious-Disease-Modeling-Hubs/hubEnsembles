---
title: "`hubEnsembles`: Ensembling Methods in R"
output: 
bibliography: ../vignettes/references.bib
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.width = 12,
  fig.height = 9
)
```

# Introduction - Emily

Predictions of future outcomes are essential to planning and decision making. Despite their utility, generating reliable predictions of the future is challenging. One method for overcoming this challenge is combining predictions across multiple, independent models. These combination methods (also called aggregation or ensembling) have been repeatedly shown to produce predictions that are more accurate [@clemen1989; @timmermann2006b] and more consistent [@hibon2005] than individual models. Because of the clear performance benefits, multi-model ensembles are commonplace across fields, including weather [@alley2019], climate [@tebaldi2007], and economics [@aastveit2018]. More recently, multi-model ensembles have been used to improve predictions of infectious disease outbreaks, including for respiratory viruses (influenza [@mcgowan2019], SARS-CoV-2 [@cramer2022]), vector borne diseases (Dengue [@johansson2019]), and hemorrhagic fevers (Ebola [@viboud2018]). <!--#EH: I DON'T LOVE THE EMPHASIS ON DISEASE APPLICATIONSâ€¦ THOUGHTS ON WHETHER TO LEAVE THIS SENTENCE, SIMPLIFY IT, OR CUT IT?--> <!--# LS: We could include infectious diseases in the list of different fields that use ensembles, then move this sentence further down to around the mention of the hubverse since its goal and our main expertise lies in infectious disease forecasting.-->

Across this vast literature, there are many proposed methods for generating ensembles. Generally, these methods differ in at least one of two ways: (1) the function used to combine or "average" predictions, and (2) how predictions are weighted when performing the combination. No one method is universally "the best"; a simple average of predictions works surprisingly well across a range of settings [@winkler2015], but more complex approaches have also been shown to have benefits [REF]. Here, we present the `hubEnsembles` package, which provides a flexible framework for generating ensemble predictions from multiple models. Complimenting an existing R package for generating ensembles of point estimates [@weiss2019], `hubEnsembles` supports multiple types of predictions, from point estimates to probabilistic predictions <!--#EH: DOES ANYONE KNOW OF OTHER PACKAGES?-->.

The `hubEnsembles` package is part of a larger collection of open-source software and data tools that enables collaborative forecasting exercises called the "hubverse". The broader "hubverse" initiative is motivated by the demonstrated benefits of collaborative hubs [@reich2022], including performance improvements of multi-model ensembles, and the desire for standardization across such hubs. In this paper, we focus specifically on the functionality encompassed in `hubEnsembles`. We provide an overview of the methods implemented (Section 2), give simple examples to demonstrate the functionality (Section 3) and a more complex case study (Section 4) that motivates a discussion and comparison of the various methods (Section 5).

<!--#EH GENERAL COMMENT: IF YOU HAVE BETTER REFERENCES ANYWHERE, LET ME KNOW.-->

# Mathematical definitions and properties of ensemble methods

The `hubEnsembles` package supports both point predictions and probabilistic predictions of different formats. A point prediction, $x$, is a single estimate of future outcomes, and a probabilistic prediction gives probabilities for a range of future outcomes specified by a distribution, $f(x)$ over future outcomes $x$.

For a set of point predictions, $x_i$ each from an distinct model $i$, the `hubEnsembles` package can compute an ensemble of these predictions

$$
x_E = C(x_i, w_i) 
$$

using any function $C$, and model-specific weights $w_i$. For example, an arithemtic average of predictions yields $x_E = \sum_{i=1}^Nx_iw_i$, where $N$ is the number of predictions and $\sum_{i=1}^Nw_i=1$. If $w_i = 1/N$ for all $i$, all predictions will be equally weighted. This framework can also support more complex functions for aggregation, such as ADD HERE.

For probabilistic predictions, there exist two such classes of methods to average or ensemble multiple predictions: quantile averaging [@lichtendahl2013] (also called a Vincent average [@vincent1912]) and probability averaging [@lichtendahl2013] also called a distributional mixture [REF?] or linear opinion pool [@stone1961]). Let $F(x)$ be a cumulative density function (CDF) defined over values $x$, and $F^{-1}(\theta)$ be the corresponding quantile function defined over quantiles $\theta$. Then, the quantile average is calculated as

$$
F^{-1}_Q(x) = \sum_{i = 1}^Nw_iF^{-1}_i(\theta)
$$

across all individual predictions, $F^{-1}_i(\theta)$, for $N$ total predictions, and $w_i$ weights for each prediction. In other words, this computes the average value across predictions for a fixed quantile. Conversely, the probability average or linear pool is calculated by averaging quantiles across predictions for a fixed value, or

$$
F_{LOP}(x) = \sum_{i = 1}^Nw_iF_i(x)
$$

again for individual predictions, $F_i(x)$, $N$ total predictions, and weights $w_i$.

The different averaging methods for probabilistic predictions yield different properties of the resulting ensemble distribution. For example, the variance of the linear pool is $\sigma^2_{LOP} = \sum_{i=1}^Nw_i\sigma_i^2 + \sum_{i=1}^Nw_i(\mu_i-\mu_{LOP})^2$, where $\mu_i$ is the mean and $\sigma^2_i$ is the variance of individual prediction $i$, and although there is no closed-form variance for the quantile average, the variance of the quantile average will always be less than or equal to that of the linear pool [@lichtendahl2013]. The linear pool method preserves variation between individual models, whereas the quantile average cancels away this variation [@howerton2023]. Both methods generate distributions with the same mean, $\mu_Q = \mu_{LOP} = \sum_{i=1}^Nw_i\mu_i$, which is the mean of individual model means.

# Model implementation details

The hubverse standardizes some common forecasting conventions, and each package within the larger suite of tools follows the set of outlined rules. We begin with a short overview of concepts needed to understand and utilize the `hubEnsembles` package, then explain the implementation of each ensembling function.

## Hubverse terminology and conventions

One may think of the model output as a central concept to the `hubEnsembles` package. A model output generally refers to a specially formatted tabular representation of forecasts produced by a modeling team. Each row represents a single, unique prediction with each column providing information about what is being forecast, its scope, and its value. The columns may be broken down into task IDs, model output representation, and, optionally, the model_id [@hubverse_docs].

The task IDs (also called task ID variables) together can be thought of as specifying the desired outcome being forecast/what the forecasts are trying to predict. They may include additional information like any conditions, assumptions, or quantitative outcomes of interest. For example, forecasts aiming to predict incident flu hospitalizations in the US at different amounts of time in the future might split up this information into a "incident flu hospitalizations" target column, a location column, and a horizon column, all of which are considered task ID columns [@hubverse_docs].

Each modeling effort may specify their forecasting goals differently within the task ID framework since the hubverse does not enforce uniformity for task ID columns. This allows for flexibility in how information about forecasting goals is split up into task ID columns and in the naming of columns. Then, the user provides task ID column names to functions that require them, like those in `hubEnsembles`. Additional examples of task ID variables are available on the [hubverse documentation website](https://hubdocs.readthedocs.io/en/latest/format/tasks.html).

The model output representation specifies how the forecasts are being conveyed and consists of three columns: (1) output_type, (2) output_type_id, and (3) value. The output_type defines how the predictive distribution is represented and may be one of mean or median (point forecasts), quantile, cdf, or pmf. The output_type_id provides more identifying information for a forecast and is specific to the particular output_type. The value contains the actual numerical prediction. Additionally, unlike for the task IDs, these three columns are required and their names are fixed [@hubverse_docs].

<!-- # LS: Maybe we should include the same summary table from the hubverse docs here https://hubdocs.readthedocs.io/en/latest/user-guide/model-output.html -->

The model_id column gives a unique identification a model that created the forecasts as a concatenation of a team abbreviation and a model abbreviation. This column may be omitted if the model outputs are from a single model only; however, is useful to denote which model made a forecast if multiple model outputs are combined into a single table. Such a table of combined model outputs is fed into an ensembling function from `hubEnsembles` to create ensemble forecasts, and these functions require the model_id column [@hubverse_docs].

## Simple ensemble

The `simple_ensemble` function directly computes an ensemble from component model outputs by combining via some function ($f$) them over a unique combination of task ID variables, output types, and output type IDs. This function can be used to summarize predictions of output types mean, median, quantile, CDF, and PMF; calculation of the ensemble is the same for each of the output types and an aggregation function of choice may be specified by the user.

The default aggregation function is mean, so `simple_ensemble` computes a mean (unweighted) ensemble by default; this corresponds to $x_E = \frac{1}{N}\sum_{i=1}^Nx_i$ for mean and median output types. Applying `simple_ensemble` (with weights) to a quantile output type is the definitional calculation of a (weighted) quantile average, $F^{-1}_Q(\theta) = \sum_{i=1}^N w_iF^{-1}_i(\theta)$. A median ensemble may also be created by specifying "median" as the aggregation function, or a custom function may be passed to the `agg_fun` argument to create other ensemble types. Similarly, weights can be specified, and if not specified, `simple_ensemble` defaults to equal weighting.

## Linear pool

The `linear_pool` function implements the linear opinion pool method for ensembling projections. This function can be used to combine predictions with output types mean, quantile, CDF, and PMF. Unlike for `simple_ensemble`, this function handles its computation differently based on the output type.

For the CDF, PMF, and mean output types, the linear pool method is equivalent to calling `simple_ensemble` with a mean aggregation function. This (weighted) mean of component model outputs is the definitional calculation for a linear pool of a CDF or PMF (see [Mathematical definitions and properties of ensemble methods]), and the mean of the LOP is the (weighted) mean of the means of the component distributions, $\mu_{LOP} = \sum_{i=1}^Nw_i\mu_i$.

However, implementation of LOP is less straightfoward for the quantile output type. The reason for this is that LOP averages quantiles across fixed values, but the predictions are provided for fixed quantiles. Thus, we must first obtain an estimate of the CDF for each component distribution using the provided quantiles, combine the CDFs, then calculate the quantiles from the ensemble's CDF. We perform this calculation in three main steps, assisted by `distfromq::make_q_fun` for the first two:

1.  Interpolate and extrapolate from the provided quantiles for each component model to obtain an estimate of the CDF of that particular distribution.
2.  Draw samples from each component model distribution. To reduce Monte Carlo variability, we use pseudo-random samples corresponding to quantiles of the estimated distribution.
3.  Pool the samples from all component models and extract the desired quantiles.

The `make_q_fun` function uses a monotonic cubic spline for interpolation of the interior while the user may choose one of several distributions to perform extrapolation of the CDF tails. These include normal, lognormal, and cauchy distributions, with "normal" set as the default.

# Demonstration of functionality - Emily

The `hubEnsembles` package includes functionality for aggregating model outputs, such as forecasts or projections, that are submitted to a hub by multiple models and combined into ensemble model outputs. The package includes two main functions: `simple_ensemble` and `linear_pool`. We illustrate these functions in this vignette, and briefly compare them.

This vignette uses the following R packages:

```{r setup}
library(dplyr)
library(tidyr)
library(ggplot2)
library(scales)
library(cowplot)
library(hubUtils)
library(hubEnsembles)
```


## Example data: a simple forecast hub

The `example-simple-forecast-hub` has been created by the Consortium of Infectious Disease Modeling Hubs as a simple example hub to demonstrate the set up and functionality for the hubverse. The hub includes both example model output data and target data. The model output data includes `quantile`, `mean` and `median` forecasts of future incident hospitalizations, as well as `pmf` forecasts of the probability that the change in hospitalizations will bee a "large decrease", "decrease", "stable", "increase", "large increase". 

<!-- # EH: A few notes on this: 
(1) I modified the example hub data to include mean and pmf output types. The mean output type was estimated from 1E4 samples (using distfromq package). The pmf output type was modeled after FluSight (probability of "large_decrease", "decrease", "stable", etc. but the values were randomly generated from a uniform distribution. Does this sound okay? Is it a problem that this won't match the example-hub directly?

(2) As of now, I have not included the cdf output type for inc hosp forecasts because it is difficult to define standard bins. I do, however, see value in including an example with cdf output type. So two potential options for that: 
    (a) include another target (e.g., peak timing) of cdf output type in this example hub 
    (b) create a very simple example, say with normal distributions, that will illustrate the fundamental differences between quantile average and linear pool (similar to Fig 2 in my paper, https://royalsocietypublishing.org/doi/10.1098/rsif.2022.0659) I have always found this figure useful, but maybe it is just me! I also think this option could help reinforce the connection between the methods described in the Mathematical Definitions section, and the implementation of each function (i.e., simple_ensemble() applied to quantile is a different method than simple_ensemble() applied to cdf/pmf).
) -->

First we load the example model output data and the target data using the `connect_hub()` function from `hubUtils`, another package developed by the Consortium of Infectious Disease Modeling Hubs.

```{r}
hub_path <- system.file("example-data/example-simple-forecast-hub",
                        package = "hubEnsembles")

model_outputs <- hubUtils::connect_hub(hub_path) %>%
  dplyr::collect()
head(model_outputs)

target_data_path <- file.path(hub_path, "target-data",
                              "covid-hospitalizations.csv")
target_data <- read.csv(target_data_path) %>%
    mutate(time_idx = as.Date(time_idx))
head(target_data)
```

## Creating ensembles with `simple_ensemble`

We can generate an equally weighted mean ensemble for each unique combination of `task_id` variables (here, `origin_date`, `horizon`, `location`, and `target`), as well as for each unique `output_type` and `output_type_id`. For the `quantile` output type in our example data, this means the resulting ensemble will be the mean of component model submitted values for each quantile. 

```{r}
mean_ens <- hubEnsembles::simple_ensemble(model_outputs)
head(mean_ens)
```

### Changing the aggregation function

We can change the function used to aggregate across model outputs. For example, we may want to calculate a median of component model submitted values for each quantile. We do so by specifying `agg_fun = median`. We will also use `model_id = "hub-ensemble-median` to change the name of this ensemble in the resulting data frame.

```{r}
median_ens <- hubEnsembles::simple_ensemble(model_outputs, 
                                            agg_fun = median, 
                                            model_id = "hub-ensemble-median")
head(median_ens)
```
Custom functions can also be passed into the `agg_fun` argument. For example, a geometric mean may be a more appropriate way to combine component model outputs; here we define a custom function `geometric_mean()` to do so. Any custom function to be used requires an argument `x` for the vector of numeric values to summarize, and if relevant, an argument `w` of numeric weights. Then, we can use this custom function to ensemble the component model outputs, again using `agg_fun = geometric_mean`.

```{r}
geometric_mean <- function(x){
    n <- length(x)
    return(prod(x)^(1/n))
}

geometric_mean_ens <-  hubEnsembles::simple_ensemble(model_outputs, 
                                            agg_fun = geometric_mean, 
                                            model_id = "hub-ensemble-geometric")
head(geometric_mean_ens)
```

As expected, the mean, median, and geometric mean each give us slightly different resulting ensembles. The median point estimates in Figure XX demonstrate this. 

```{r}
plt_all_point_ens <- bind_rows(mean_ens, 
                               median_ens, 
                               geometric_mean_ens) %>% 
    filter(location == "US", 
           origin_date == "2022-12-12", 
           output_type == "median") %>%
    mutate(target_date =  origin_date + horizon, 
           ens_flag = model_id)

plt_all_point_mod <- model_outputs %>% 
    filter(location == "US", 
           origin_date == "2022-12-12", 
           output_type == "median") %>%
    mutate(target_date =  origin_date + horizon)

ggplot(data = plt_all_point_ens) + 
    geom_point(data = target_data %>%
                   filter(location == "US", 
                          time_idx >= "2022-10-01",
                          time_idx <= "2023-03-01"),
               aes(x = time_idx, y = value)) +
    geom_line(data = target_data %>%
                   filter(location == "US", 
                          time_idx >= "2022-10-01",
                          time_idx <= "2023-03-01"),
               aes(x = time_idx, y = value)) +
    geom_vline(aes(xintercept = as.Date("2022-12-06"))) +
    geom_line(data = plt_all_point_mod, 
              aes(x = target_date, y = value, color = model_id), size = 0.9) + 
    geom_line(aes(x = target_date, y = value, color = model_id), size = 0.9) + 
    facet_grid(cols = vars(ens_flag)) +
    scale_color_manual(values = c(rainbow(3), gray(seq(0.2, 0.8,length.out = 3)))) +
    scale_y_continuous(labels = comma, 
                       name = "US incident hospitalizations") +
    theme_bw() + 
    theme(axis.title.x = element_blank(), 
          legend.position = "bottom", 
          legend.title = element_blank())
```

<!-- # EH: I don't love the color scheme here... it's especially hard to see the baseline model. -->

### Weighting model contributions

In addition, we can weight the contributions of each model by providing a `data.frame` that specifies the weights for each model. For example, if we wanted to include the baseline model in the ensemble, but give it less weight than the other forecasts, we would use the `weights = model_weights` argument, where `model_weights` is a `data.frame` with a `model_id` column containing each unique model_id and a `weight` column.

```{r}
model_weights <- data.frame(model_id = c("UMass-ar", "UMass-gbq", "simple_hub-baseline"), 
                            weight = c(0.4, 0.4, 0.2))

weighted_mean_ens <- hubEnsembles::simple_ensemble(model_outputs, 
                                                   weights = model_weights, 
                                                   model_id = "hub-ensemble-weighted-mean")
head(weighted_mean_ens)
```
If the `data.frame` defines model weights in a column named something other than `weight`, we can specify this using the `weights_col_name` argument. 

```{r}
model_weights <- data.frame(model_id = c("UMass-ar", "UMass-gbq", "simple_hub-baseline"), 
                            w = c(0.4, 0.4, 0.2))

weighted_mean_ens <- hubEnsembles::simple_ensemble(model_outputs, 
                                                   weights = model_weights, 
                                                   weights_col_name = "w",
                                                   model_id = "hub-ensemble-weighted-mean")
head(weighted_mean_ens)
```

## Creating ensembles with `linear_pool`

We can also generate a linear pool ensemble, or distributional mixture, using the `linear_pool()` function; this function can be applied to predictions with an `output_type` of `mean`, `quantile`, `cdf`, or `pmf`. Our example hub includes `median` output type, so we exclude it from the calculation. 




<!-- # EH: I needed to add the `as.numeric(output_type_id)` line to get this function to work because the model_outputs data.frame has output_types of pmf and quantile (so the output_type_id column is chr), and the distfromq function for the quantile output type cannot use character quatnile values. I have added an issue for this. -->


```{r}
linear_pool_ens <- hubEnsembles::linear_pool(model_outputs %>%
                                               filter(output_type != "median") %>%
                                               mutate(output_type_id = as.numeric(output_type_id)), 
                                             model_id = "hub-ensemble-linear-pool")
head(linear_pool_ens)
```

For `quantile` model outputs, the `linear_pool` function approximates a full probability distribution using the value-quantile pairs from each component model. As a default, this is done with functions in the `distfromq` package, which defaults to fitting a monotonic cubic spline. To increase the number of samples drawn from the distribution of each component model defaults to `1e4`, but this can be changed using the `n_samples` argument. 

For `mean`, `cdf` and `pmf` output types, the linear pool is equivalent to using a mean `simple_ensemble` (see Mathematical definitions and properties for explanation). For `quantile` output types, the `linear_pool` function yields an ensemble with wider prediction intervals than `simple_ensemble`. 

```{r}
plt_model_outputs = model_outputs %>%
    filter(location == "US", 
           origin_date == "2022-12-12", 
           output_type == "quantile", 
           output_type_id %in% c(0.05, 0.25, 0.5, 0.75, 0.95)) %>%
    mutate(target_date =  origin_date + horizon, 
           output_type_id = paste0("Q", as.numeric(output_type_id)*100)) %>%
    spread(output_type_id, value)
    
plt_ens_outputs <- bind_rows(
    mean_ens %>% 
        filter(location == "US", 
               origin_date == "2022-12-12", 
               output_type == "quantile", 
               output_type_id %in% c(0.05, 0.25, 0.5, 0.75, 0.95)) %>%
        mutate(target_date =  origin_date + horizon, 
               output_type_id = paste0("Q", as.numeric(output_type_id)*100), 
               ens_fn = "simple_ensemble"), 
    linear_pool_ens %>% 
        filter(location == "US", 
               origin_date == "2022-12-12", 
               output_type == "quantile", 
               output_type_id %in% c(0.05, 0.25, 0.5, 0.75, 0.95)) %>%
        mutate(target_date =  origin_date + horizon, 
               output_type_id = paste0("Q", as.numeric(output_type_id)*100), 
               ens_fn = "linear_pool")
) %>%
    spread(output_type_id, value)

lims = c(min(c(plt_model_outputs$Q5, plt_ens_outputs$Q5)), 
         max(c(plt_model_outputs$Q95, plt_ens_outputs$Q95)))

p1 = ggplot(data = plt_model_outputs, 
            aes(x = target_date)) + 
    geom_point(data = target_data %>%
                   filter(location == "US", 
                          time_idx >= "2022-10-01",
                          time_idx <= "2023-03-01"),
               aes(x = time_idx, y = value)) +
    geom_line(data = target_data %>%
                  filter(location == "US", 
                         time_idx >= "2022-10-01",
                         time_idx <= "2023-03-01"),
              aes(x = time_idx, y = value)) +
    geom_vline(aes(xintercept = as.Date("2022-12-06"))) +
    geom_ribbon(aes(ymin = Q5, ymax = Q95), alpha = 0.2) + 
    geom_ribbon(aes(ymin = Q25, ymax = Q75), alpha = 0.2) + 
    geom_line(aes(y = Q50), size = 0.9) +
    facet_wrap(vars(model_id), ncol = 1)+
    scale_y_continuous(labels = comma, 
                       limits = lims,
                       name = "US incident hospitalizations") +
    theme_bw() +
    theme(axis.title.x = element_blank(), 
          legend.position = "none", 
          legend.title = element_blank())
    
p2 = ggplot(data = plt_ens_outputs, 
            aes(x = target_date)) + 
    geom_point(data = target_data %>%
                   filter(location == "US", 
                          time_idx >= "2022-10-01",
                          time_idx <= "2023-03-01"),
               aes(x = time_idx, y = value)) +
    geom_line(data = target_data %>%
                   filter(location == "US", 
                          time_idx >= "2022-10-01",
                          time_idx <= "2023-03-01"),
               aes(x = time_idx, y = value)) +
    geom_vline(aes(xintercept = as.Date("2022-12-06"))) +
    geom_ribbon(aes(ymin = Q5, ymax = Q95, fill = model_id), alpha = 0.2) + 
    geom_ribbon(aes(ymin = Q25, ymax = Q75, fill = model_id), alpha = 0.2) + 
    geom_line(aes(y = Q50, color = model_id), size = 0.9) +
    scale_y_continuous(labels = comma, 
                       limits = lims,
                       name = "US incident hospitalizations") +
    theme_bw() +
    theme(axis.title.x = element_blank(), 
          legend.position = "bottom", 
          legend.title = element_blank())

plot_grid(p1, p2, 
          align = "h", axis = "b")
```



<!-- # EH: I am leaving Evan's plotly code for now, in case we want it in the vignette. I opted for ggplot for the manuscript.-->

```{r}
# basic_plot_function <- function(plot_df, truth_df, plain_line = 0.5, ribbon = c(0.975, 0.025),
#                                 forecast_date) {
# 
#   plain_df <- dplyr::filter(plot_df, output_type_id == plain_line)
# 
#   ribbon_df <- dplyr::filter(plot_df, output_type_id %in% ribbon) %>%
#     dplyr::mutate(output_type_id = ifelse(output_type_id == min(ribbon),
#                                           "min", "max")) %>% 
#     tidyr::pivot_wider(names_from = output_type_id, values_from = value)
# 
#   plot_model <- plot_ly(height = 600, colors = scales::hue_pal()(50)) 
# 
#   if (!is.null(truth_df)) {
#     plot_model <- plot_model %>% 
#       add_trace(data = truth_df, x = ~time_idx, y = ~value, type = "scatter",
#                 mode = "lines+markers", line = list(color = "#6e6e6e"),
#                 hoverinfo = "text", name = "ground truth",
#                 hovertext = paste("Date: ", truth_df$time_value, "<br>", 
#                                   "Ground truth: ", 
#                                   format(truth_df$value, big.mark = ","), 
#                              sep = ""), 
#                 marker = list(color = "#6e6e6e", size = 7))
#   }
#   plot_model <- plot_model %>% 
#     add_lines(data = plain_df, x = ~target_date, y = ~value, 
#               color = ~model_id) %>% 
#     add_ribbons(data = ribbon_df, x = ~target_date, ymin = ~min, 
#                 ymax = ~max, color = ~model_id, opacity = 0.25, 
#                 line = list(width = 0), showlegend = FALSE) %>%
#     plotly::layout(shapes = list(type = "line", y0 = 0, y1 = 1, yref = "paper",
#                                 x0 = forecast_date, x1 = forecast_date,
#                                 line = list(color = "gray")))
# }
```

```{r}
# plot_df <- dplyr::bind_rows(model_outputs, mean_ens) %>%
#   dplyr::filter(location == "US", origin_date == "2022-12-12") %>%
#   dplyr::mutate(target_date = origin_date + horizon)
# 
# plot <- basic_plot_function(
#     plot_df,
#     truth_df = target_data %>%
#         dplyr::filter(location == "US",
#                       time_idx >= "2022-10-01",
#                       time_idx <= "2023-03-01"),
#     forecast_date = "2022-12-12")
# plot
```

# Case study: Weekly incident flu hospitalizations - Li

To demonstrate the utility of the `hubEnsembles` package and the differences between the two ensembling functions, we examine the case of predicting weekly influenza hospitalizations in the US. Accurate infectious disease forecasts are needed to make decisions that inform public policy, balance prevention and mitigation efforts with other monetary and societal costs, and properly allocate resources [@disease_economics]. However, making reliable predictions is hindered by four main factors unique to infectious disease forecasting: 1) the systems governing the spread of infectious disease tend to be non-stationary, 2) communicated forecasts impact the spread of infectious disease, 3) variable truth data quality, and 4) the many unknowns about the underlying disease mechanisms [@lauer]. 

The US Center for Disease Control and Prevention (CDC) has been soliciting flu forecasts from modeling teams for years through FluSight, and here we combine them using various aggregation methods to take advantage of the greater consistency [@hibon2005] and accuracy [@clemen1989; @timmermann2006b] of ensembles over individual models. In particular, we examine four equally-weighted ensembling methods implemented through `simple_ensemble()` and `linear_pool()`: a quantile (arithmetic) mean, a quantile median, a linear pool with normal tails, and a linear pool with lognormal tails.

The component forecasts used to generate the ensembles consist of those submitted during influenza seasons 2021-2022 and 2022-2023, the only two complete seasons for which FluSight collected quantile forecasts for a target of weekly incident flu hospitalizations. These predictions are not yet formatted according to hubverse standards but are easily transformed using the `as_model_out_tbl()` function from the `hubUtils` package.

Within the now-properly formatted model outputs, the task ID variables are horizon, location, target, and forecast date (the date on which the forecast was made). For model output specifications, all of the forecasts are a quantile output type with 23 total unique output type IDs $$Q = \{.010, 0.025, .050, .100, \cdots, .900, .950, .990\}.$$ The values of these quantile forecasts are non-negative. The resulting ensemble forecasts will have the same task ID variables and model output specifications.

Next the component model outputs are combined using the following code to generate each model. <!-- # LS: Should it be just for a single forecast date or mapping over all of them? -->

```{r read in forecasts and scores, echo=FALSE}
library(covidHubUtils)
library(lubridate)
library(patchwork)
source("evaluation_functions.R")

flu_truth_all <- readr::read_rds("data/flu_truth_all.rds")
flu_files <- list.files(path="data", pattern="hub", full.names=TRUE)
flu_forecasts_all <- purrr::map_dfr(flu_files, .f=readr::read_rds) |>
  as_covid_hub_forecasts(model_id_col = "model_id",
                        reference_date_col="forecast_date", 
                        location_col="location", 
                        horizon_col="horizon", 
                        target_col="target", 
                        temp_res_col=NULL, 
                        target_end_date_col="target_end_date")

flu_scores_baseline <- readr::read_rds("data/flu_baseline_scores.rds")
flu_scores_ensembles <- readr::read_rds("data/flu_scores_ensembles.rds")
flu_scores_all <- rbind(flu_scores_ensembles, flu_scores_baseline)

flu_dates_21_22 <- as.Date("2022-01-24") + weeks(0:21)
flu_dates_22_23 <- as.Date("2022-10-17") + weeks(0:30)
all_flu_dates <- c(flu_dates_21_22, flu_dates_22_23)

model_names <- c("Flusight-baseline", "mean-ensemble", "median-ensemble", "lp-normal", "lp-lognormal")
model_colors <- c("black", "#F8766D", "#B79F00", "#00BFC4", "#C77CFF")
```

We score the ensemble forecasts for every unique combination of task ID variables using weighted interval score (WIS), MAE, 50% PI coverage, and 95% PI coverage. The scores are then grouped together and averages are taken of these metrics over different combinations of five variables: none, season, horizon, forecast date, and location. We display a table of the overall results (no grouping) and figures showing average WIS, MAE, and 95% PI coverage plotted against forecast date, horizon, and season.

The summarized scores show that the quantile median ensemble has the best overall performance in terms of WIS and MAE (and the relative versions of these metrics) with above-nominal coverage rates. The two linear opinion pools had the second-best WIS and MAE values, the latter being very similar in value, but the highest 50% and 95% coverage rates. The quantile mean performed the worst of the ensembles and similarly to the baseline, only outperforming it in terms of WIS.

```{r overall evaluation, message=FALSE, warning=FALSE, echo=FALSE}
flu_overall_states <- flu_scores_all |>
  evaluate_flu_scores(grouping_variables=NULL, baseline_name="Flusight-baseline", us_only=FALSE)

knitr::kable(flu_overall_states, caption="Summary of overall model performance across both seasons, averaged over the states geographic scale.")
```

Plots of the models' forecasts can aid our understanding about the origin of these accuracy differences. For example, the linear opinion pools, which had the highest coverage rates, consistently demonstrate some of the widest intervals. The median ensemble, which had the best WIS, seems to have best-balanced interval width and point forecast accuracy among the models, with narrower intervals than the linear pools that achieved near-nominal coverage on average. The baseline, however, generated intervals that were too narrow and often inaccurate point forecasts. The quantile mean also frequently suffered from similarly narrow intervals, though at times it had intervals as wide or wider than those of the linear pools. 

```{r plot forecasts, echo = FALSE}
flu_forecasts_wide <- flu_forecasts_all |>
  dplyr::left_join(hub_locations_flusight, by = c("location" = "fips")) |>
  dplyr::mutate(horizon=as.character(horizon)) 

select_dates <- all_flu_dates[seq(1, 53, 4)]


forecasts_hi <- flu_forecasts_wide %>% 
  filter(location == "15", forecast_date %in% select_dates)
    
hi_plot <- plot_forecasts(forecasts_hi, 
                    hub = "FluSight",
                    intervals = c(.50, .95),
                    truth_data = select(flu_truth_all, model, location, target_end_date, target_variable, value),
                    truth_source = "HealthData",
                    use_median_as_point = TRUE,
                    facet = model ~.,
                    facet_nrow = 5,
                    fill_by_model = TRUE, 
                    title = "none",
                    subtitle = "none",
                    show_caption = FALSE,
                    plot=FALSE)

max_truth_hi <- max(filter(flu_truth_all, location=="15")$value)
max_forecast_hi <- max(filter(forecasts_hi, location=="15")$value)
  
hi_plot <- hi_plot +
  scale_x_date(name=NULL, limits = c(as.Date("2022-01-01"), as.Date("2023-06-08")), date_labels = "%b '%y") +
  coord_cartesian(ylim = c(0, max(25, min(max_truth_hi * 3.5, max_forecast_hi)))) +
  ggtitle("Hawaii (lowest cumulative hospitalizations)") +
  theme(axis.ticks.length.x = unit(0.5, "cm"),
        axis.text.x = element_text(vjust = 7, hjust = -0.2),
        legend.position = "none")
          

forecasts_tx <- flu_forecasts_wide %>% 
  filter(location == "48", forecast_date %in% select_dates)
    
tx_plot <- plot_forecasts(forecasts_tx, 
                    hub = "FluSight",
                    intervals = c(.50, .95),
                    truth_data = select(flu_truth_all, model, location, target_end_date, target_variable, value),
                    truth_source = "HealthData",
                    use_median_as_point = TRUE,
                    facet = model ~.,
                    facet_nrow = 5,
                    fill_by_model = TRUE, 
                    title = "none",
                    subtitle = "none",
                    show_caption = FALSE,
                    plot=FALSE)

max_truth_tx <- max(filter(flu_truth_all, location=="48")$value)
max_forecast_tx <- max(filter(forecasts_tx, location=="48")$value)
  
tx_plot <- tx_plot +
  scale_x_date(name=NULL, limits = c(as.Date("2022-01-01"), as.Date("2023-06-08")), date_labels = "%b '%y") +
  coord_cartesian(ylim = c(0, max(25, min(max_truth_tx * 3.5, max_forecast_tx)))) +
  ggtitle("Texas (highest cumulative hospitalizations)") +
  theme(axis.ticks.length.x = unit(0.5, "cm"),
        axis.text.x = element_text(vjust = 7, hjust = -0.2),
        legend.position = "none")
          

hi_plot + tx_plot +
  plot_layout(ncol = 2, guides='collect') &
  plot_annotation(
    title="Weekly Influenza Hospitalizations: observed and forecasted",
    subtitle="Forecasts for Hawaii and Texas, every 4 weeks",
  ) &
  theme(legend.position='bottom')
```

We may also examine the scores grouped by forecast date and horizon. The three sets of plots show average WIS, MAE, and 95% PI coverage for these groupings (as well as by season for readability). The models, especially the ensembles, demonstrate great similarity in terms of MAE during the entire time period for the one week ahead horizon, though the MAE values diverge at increased horizons. However, the models show greater differences for the other two metrics, especially during times of rapid change. The linear pools show greater or equal 95% coverage rates compared to that of the other models for the entire period of analysis at both plotted horizons. Notably, for WIS the linear pools outperform the median ensemble at the one week ahead horizon during these times of rapid change, something not shown in the overall analysis.

```{r By date States metrics, message=FALSE, warning=FALSE, fig.cap='Average h-week ahead WIS, MAE, and 95% PI coverage for each model for the States national level.', echo=FALSE}  
flu_date_horizon_season_states <- flu_scores_all |>
  evaluate_flu_scores(grouping_variables=c("horizon", "forecast_date", "season"), baseline_name="Flusight-baseline", us_only=FALSE)

# WIS
wis_date_plot_states1_2122 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2021-2022") |>
  plot_evaluated_scores_forecast_date(model_names, model_colors, h=1, main="WIS 2021-2022, 1 week ahead")
wis_date_plot_states4_2122 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2021-2022") |>
  plot_evaluated_scores_forecast_date(model_names, model_colors, h=4, main="WIS 2021-2022, 4 week ahead")
  
wis_date_plot_states1_2223 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2022-2023") |>
  plot_evaluated_scores_forecast_date(model_names, model_colors, h=1, main="WIS 2022-2023, 1 week ahead")
wis_date_plot_states4_2223 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2022-2023") |>
  plot_evaluated_scores_forecast_date(model_names, model_colors, h=4, main="WIS 2022-2023, 4 week ahead")

# MAE
flu_date_horizon_season_states <- flu_scores_all |>
    evaluate_flu_scores(grouping_variables=c("horizon", "forecast_date", "season"), baseline_name="Flusight-baseline", us_only=FALSE)

mae_date_plot_states1_2122 <- flu_date_horizon_season_states |>
    dplyr::filter(season == "2021-2022") |>
    plot_evaluated_scores_forecast_date(model_names, model_colors, h=1, y_var="mae", main="MAE 2021-2022, 1 week ahead")
mae_date_plot_states4_2122 <- flu_date_horizon_season_states |>
    dplyr::filter(season == "2021-2022") |>
    plot_evaluated_scores_forecast_date(model_names, model_colors, h=4, y_var="mae", main="MAE 2021-2022, 4 week ahead")

mae_date_plot_states1_2223 <- flu_date_horizon_season_states |>
    dplyr::filter(season == "2022-2023") |>
    plot_evaluated_scores_forecast_date(model_names, model_colors, h=1, y_var="mae", main="MAE 2022-2023, 1 week ahead")
mae_date_plot_states4_2223 <- flu_date_horizon_season_states |>
    dplyr::filter(season == "2022-2023") |>
    plot_evaluated_scores_forecast_date(model_names, model_colors, h=4, y_var="mae", main="MAE 2022-2023, 4 week ahead")

# 95% Coverage
cov95_date_plot_states1_2122 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2021-2022") |>
  plot_evaluated_scores_forecast_date(model_names, model_colors, h=1, y_var="cov95", main="95% Coverage 2021-22, 1 week ahead") +
  coord_cartesian(ylim = c(0, 1.05))
cov95_date_plot_states4_2122 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2021-2022") |>
  plot_evaluated_scores_forecast_date(model_names, model_colors, h=4, y_var="cov95", main="95% Coverage 2021-22, 4 week ahead") +
  coord_cartesian(ylim = c(0, 1.05))
  
cov95_date_plot_states1_2223 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2022-2023") |>
  plot_evaluated_scores_forecast_date(model_names, model_colors, h=1, y_var="cov95", main="95% Coverage 2022-23, 1 week ahead") +
  coord_cartesian(ylim = c(0, 1.05))
cov95_date_plot_states4_2223 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2022-2023") |>
  plot_evaluated_scores_forecast_date(model_names, model_colors, h=4, y_var="cov95", main="95% Coverage 2022-23, 4 week ahead") +
  coord_cartesian(ylim = c(0, 1.05))
    

wis_date_plot_states1_2122 + mae_date_plot_states1_2122 + cov95_date_plot_states1_2122 + 
  wis_date_plot_states4_2122 + mae_date_plot_states4_2122 + cov95_date_plot_states4_2122 + 
  wis_date_plot_states1_2223 + mae_date_plot_states1_2223 + cov95_date_plot_states1_2223 + 
  wis_date_plot_states4_2223 + mae_date_plot_states4_2223 + cov95_date_plot_states4_2223 + 
  plot_layout(ncol=3, guides='collect') &
  theme(legend.position='bottom')
```

Grouping the scores by horizon on its own and horizon in conjunction with season reveal similar patterns in model rankings, with the quantile median achieving the lowest WIS except at the 1 week ahead horizon. When the scores are grouped by only location, the quantile median has the lowest relative WIS for all but three locations, two of which demonstrate inferior performance compared to the baseline and one which has worse performance than the quantile mean. The linear pools outperform the quantile mean ensemble for 75% of locations and perform the same for about 8%, each of these three models outperforming the baseline for approximately 75% of locations.

From these results, we can see that different ensembling methods perform best under different circumstances, though all outperform the baseline model. While the quantile median demonstrates the best overall results for WIS, MAE, 50% PI coverage, and 95% PI coverage, other models may perform better from week-to-week for each metric. For example, the linear pools achieve the lowest WIS during several instances of rapid change in incident flu hospitalizations.

Depending on the target being forecast or goal of forecasting, the user may opt to choose a particular aggregation method most aligned with the objectives. One case may call for prioritizing above-nominal coverage rates while another may be more interested in accurate point forecasts. The `simple_ensemble` and `linear_pool` functions and the ability to specify component model weights and an aggregation function for `simple_ensemble` allow users to implement a variety of ensemble methods.

In this case study, we have demonstrated how to utilize `hubEnsembles` to combine individual model forecasts to create more accurate ensemble predictions for weekly influenza hospitalizations in the US. The simple and flexible framework can generate multiple types of ensembles for a range of output types, making this package a valuable addition to the existing software.


# Conclusion/Discussion - Undecided
